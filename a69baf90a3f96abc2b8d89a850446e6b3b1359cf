{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "de84a0df_83fed9ad",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-13T19:53:42Z",
      "side": 1,
      "message": "Dmitriy,\n\nAs I was playing with threading the block jacobi preconditioner your observation from the threaded matrix multiplication CL showed up, i.e. the single threaded performance using parallelfor is worse (significantly so) compared to just using the loop directly.\n\nAnd as I was reviewing this CL, it made me think about the chunking you are doing here where multiple work items that are consecutive are executed on the same thread.\n\nI am wondering if this and the problem of num_threads \u003d1 can be solved in a unified fashion. Here is my thought.\n\nCurrently the way parallelfor is structured is as follows:\n\nvoid ParallelFor(ContextImpl* context,\n                 int start,\n                 int end,\n                 int num_threads,\n                 const std::function\u003cvoid(int thread_id, int i)\u003e\u0026 function)\n                 \nwhere the expectation is that function operates on a single element i.\nwhat if we instead took \n\n",
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "65900019_def59513",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-13T19:56:55Z",
      "side": 1,
      "message": "oops early reply.\n\ninstead if we took \n\nconst std::function\u003cvoid(int thread_id, int start, int end)\u003e\u0026 function\n\nthis would solve the num_threads\u003d1 case, and also allow us to use variable sized chunks ? WDYT? \n\nwe are not creating a threadpool for general usage, we are doing this for our purposes, so we can futz with the calling API as we need.\n\nFurther, we understand our workloads, generally speaking our workloads are small per loop iteration -- unless someone is doing really heavy duty numerical differentiation in their jacobian evaluation.",
      "parentUuid": "de84a0df_83fed9ad",
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "f83b8171_43c58bbc",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 6141
      },
      "writtenOn": "2022-09-13T21:00:39Z",
      "side": 1,
      "message": "Yes, I think it is a good idea; I will implement this tomorrow.",
      "parentUuid": "65900019_def59513",
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "b504a520_62fd3232",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 6141
      },
      "writtenOn": "2022-09-13T21:00:39Z",
      "side": 1,
      "message": "Let me explain the most confusing part of the change-set in a more explicit way.\n\n\nProbably it is better to start with a small concrete example:\n\nLet say we want to split 14 items into blocks for 4 threads, with a single item taking 1s to process\n\na. We split using proposed algo:[4, 4, 3, 3] \u003d\u003e total runtime is 4\nb. We split into equal-sized blocks and remainder: [3, 3, 3, 3, 2] \u003d\u003e total runtime is 5 (3 for 4 blocks of 3 in parallel, and then wait for a single thread processing remaining block)\nc. We split into equal-sized blocks and then split remainder into equal-sized blocks: [3,3,3,3,1,1] \u003d\u003e total runtime is 4, but we have more synchronization events than in case a.\n\nThings will get a bit more dramatic in case of 31 items and 16 threads:\na: Proposed: [2,2,...,2,1] \u003d\u003e total run time 2, 16 blocks\nb: One block for remainder: [1,...1, 15] \u003d\u003e total runtime 16\nc: Splitting remainder into 1-sized blocks: [1,...,1] \u003d\u003e total runtime 2, 31 blocks\n\n\n\nMain idea is to slightly adjust (by 1 extra element at most) several blocks and keep number of blocks being a multiple of number of threads instead of adding extra block(s).",
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c2efbd42_73f1f52a",
        "filename": "internal/ceres/parallel_for_cxx.cc",
        "patchSetId": 2
      },
      "lineNbr": 63,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-13T19:44:03Z",
      "side": 1,
      "message": "It is better to have an explicit comparison with zero.",
      "range": {
        "startLine": 63,
        "startChar": 9,
        "endLine": 63,
        "endChar": 26
      },
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "bc1e487d_b34a75d0",
        "filename": "internal/ceres/parallel_for_cxx.cc",
        "patchSetId": 2
      },
      "lineNbr": 82,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-13T19:44:03Z",
      "side": 1,
      "message": "the variable names are self explanatory now, you can drop the comments.",
      "range": {
        "startLine": 82,
        "startChar": 24,
        "endLine": 82,
        "endChar": 26
      },
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "1a01acbe_fc5788e0",
        "filename": "internal/ceres/parallel_for_cxx.cc",
        "patchSetId": 2
      },
      "lineNbr": 101,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-13T19:44:03Z",
      "side": 1,
      "message": "divisible",
      "range": {
        "startLine": 101,
        "startChar": 54,
        "endLine": 101,
        "endChar": 63
      },
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "e72a06d5_7d46bf28",
        "filename": "internal/ceres/parallel_for_cxx.cc",
        "patchSetId": 2
      },
      "lineNbr": 104,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-13T19:44:03Z",
      "side": 1,
      "message": "this seems very confusing to me.\n\nas I understand it\n\nif num_work_blocks divideds (end - start) then you will have a total of num_work_blocks each of size (end - start)/num_work_blocks.\n\nif num_work_blocks does NOT divide (end - start) then you will have \n\n(num_work_blocks) blocks of size floor((end-start)/num_work_blocks) \n\nand and extra block of size (end - start) % num_work_blocks no?",
      "range": {
        "startLine": 104,
        "startChar": 45,
        "endLine": 104,
        "endChar": 48
      },
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "228f38fc_497b13bf",
        "filename": "internal/ceres/parallel_for_cxx.cc",
        "patchSetId": 2
      },
      "lineNbr": 104,
      "author": {
        "id": 6141
      },
      "writtenOn": "2022-09-13T21:00:39Z",
      "side": 1,
      "message": "No, this way extra block might end up being processed by a single thread while all other threads already finished working (because initial number of blocks was divisible by number of threads).\n\nThus we spread remainder over multiple blocks, adding one additional index per block.",
      "parentUuid": "e72a06d5_7d46bf28",
      "range": {
        "startLine": 104,
        "startChar": 45,
        "endLine": 104,
        "endChar": 48
      },
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d7a54096_d541b0fc",
        "filename": "internal/ceres/parallel_for_cxx.cc",
        "patchSetId": 2
      },
      "lineNbr": 104,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-13T21:36:55Z",
      "side": 1,
      "message": "okay I understand now.",
      "parentUuid": "228f38fc_497b13bf",
      "range": {
        "startLine": 104,
        "startChar": 45,
        "endLine": 104,
        "endChar": 48
      },
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "f6db8130_2bdfcf86",
        "filename": "internal/ceres/parallel_for_cxx.cc",
        "patchSetId": 2
      },
      "lineNbr": 114,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-13T19:44:03Z",
      "side": 1,
      "message": "these definitions are reversed\nwhat you are defining to be tail_block_size is really the size of the \"normal\" block.\nand what you are defining as num_full_sized_blocks is the size of the last remainder block no?",
      "range": {
        "startLine": 114,
        "startChar": 8,
        "endLine": 114,
        "endChar": 23
      },
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "e47af04b_ae3db4bd",
        "filename": "internal/ceres/parallel_for_cxx.cc",
        "patchSetId": 2
      },
      "lineNbr": 114,
      "author": {
        "id": 6141
      },
      "writtenOn": "2022-09-13T21:00:39Z",
      "side": 1,
      "message": "When splitting N elements into M blocks we will have:\nN % M \"front\" blocks of size floor(N / M) + 1\nM - (N % M) \"tail\" blocks of size floor(N / M)\n\nThis is done in order to minimize max | n_i - n_j |  where n_i is number of items in i-th block, while keeping number of blocks exactly M.",
      "parentUuid": "f6db8130_2bdfcf86",
      "range": {
        "startLine": 114,
        "startChar": 8,
        "endLine": 114,
        "endChar": 23
      },
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d9137598_78375a3a",
        "filename": "internal/ceres/parallel_for_cxx.cc",
        "patchSetId": 2
      },
      "lineNbr": 114,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-13T21:36:55Z",
      "side": 1,
      "message": "I understand now.\n\nin light of this, the variable names I suggested are not great.\n\nhow about\n\ntail_block_size -\u003e base_block_size;\nnum_full_size_blocks -\u003e num_plus1_sized_blocks;",
      "parentUuid": "e47af04b_ae3db4bd",
      "range": {
        "startLine": 114,
        "startChar": 8,
        "endLine": 114,
        "endChar": 23
      },
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "44b35813_aa1bd04f",
        "filename": "internal/ceres/parallel_for_cxx.cc",
        "patchSetId": 2
      },
      "lineNbr": 136,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-13T19:44:03Z",
      "side": 1,
      "message": "give this a better name.",
      "range": {
        "startLine": 136,
        "startChar": 20,
        "endLine": 136,
        "endChar": 21
      },
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "aabd6ed6_0c09bc89",
        "filename": "internal/ceres/parallel_for_cxx.cc",
        "patchSetId": 2
      },
      "lineNbr": 215,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-13T19:44:03Z",
      "side": 1,
      "message": "shouldn\u0027t the early exit for end \u003c\u003d start be at the very top? regardless of whether it is one thread or more.",
      "range": {
        "startLine": 215,
        "startChar": 10,
        "endLine": 215,
        "endChar": 11
      },
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "7aa13a2d_bd3dcaa4",
        "filename": "internal/ceres/parallel_for_cxx.cc",
        "patchSetId": 2
      },
      "lineNbr": 247,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-13T19:44:03Z",
      "side": 1,
      "message": "I think i here and in shared_state should be named to block_id",
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "2e0655ad_b902af2a",
        "filename": "internal/ceres/parallel_for_cxx.cc",
        "patchSetId": 2
      },
      "lineNbr": 254,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-13T19:44:03Z",
      "side": 1,
      "message": "this is wrong, only the last block is sized tail_block_size if there is one.\n\nI think you have mixed up tail_block with the regular blocks in the comments and in the code.",
      "range": {
        "startLine": 254,
        "startChar": 44,
        "endLine": 254,
        "endChar": 59
      },
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "8282f8b3_dd70d848",
        "filename": "internal/ceres/parallel_for_cxx.cc",
        "patchSetId": 2
      },
      "lineNbr": 254,
      "author": {
        "id": 6141
      },
      "writtenOn": "2022-09-13T21:00:39Z",
      "side": 1,
      "message": "When splitting N elements into M blocks we will have:\n\nM \"tail\" blocks when N % M \u003d\u003d 0, and 0 \"front\" blocks\nM - 1 \"tail\" blocks when N % M \u003d\u003d 1, and 1 \"front\" blocks\n...\nM - K \"tail\" blocks when N % M \u003d\u003d K, and K \"front\" blocks\n\nEvery \"tail\" block have size floor(N / M), every \"front\" (\u003d \"full sized\") block have size floor(N / M) + 1\n\n\nThe goal is to create exactly M blocks, not M + 1 blocks; because M is divisible by number of threads, and adding one extra block will lead to waiting a single thread to process this last block (while all other threads already stopped).",
      "parentUuid": "2e0655ad_b902af2a",
      "range": {
        "startLine": 254,
        "startChar": 44,
        "endLine": 254,
        "endChar": 59
      },
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a3abb801_65c95d9f",
        "filename": "internal/ceres/parallel_for_cxx.cc",
        "patchSetId": 2
      },
      "lineNbr": 254,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-13T21:36:55Z",
      "side": 1,
      "message": "Thanks for the explanation, I agree this is better. I think with a slightly different variable naming this will be much better.",
      "parentUuid": "8282f8b3_dd70d848",
      "range": {
        "startLine": 254,
        "startChar": 44,
        "endLine": 254,
        "endChar": 59
      },
      "revId": "a69baf90a3f96abc2b8d89a850446e6b3b1359cf",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    }
  ]
}