{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "67d0d2b9_797e0232",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 6141
      },
      "writtenOn": "2022-10-24T00:22:44Z",
      "side": 1,
      "message": "This is an extension of https://ceres-solver-review.googlesource.com/c/ceres-solver/+/22740 .\n\nI\u0027ve added routines for maintaining transpose block structure to AppendRows and DeleteRowBlocks functions and corresponding tests.\n\n\nWhile testing a simple approach to parallelization of left multiplications on the BAL dataset, I\u0027ve found that results do not follow along results of `spmv_benchmark`.\nI think that this is due to significantly different sparsity pattern column-wise:\n - In real data most points have single-digit number of projections and number of points with a certain projection count decreases rapidly\n - In our benchmark with point-camera association being distributed according to Bernoulli distribution, number of projections is distributed by binomial distribution. With 1000 cameras and 0.1 visibility probability this gives [82, 119] 95% confidence interval for number of projections per point.\n\n\nThus I changed algorithm for parallel left multiplying to one that works better on BAL data.\n\nThe idea is to calculate number of non-zeros in each column-block and split work +- equally between threads.\n\n\nAs shown in the table below, this leads to ~2x total speedup of iterative linear solving (and results in 5.8..8x slowdown compared to CUDA implementation.\n\nWhile it is relatively close to memory bandwidth ratio, I still think there is some room for improvement, but I would like to ask a couple of design questions:\n - Do you think it is a good idea to store at least a total number of non-zeros in column-block (and maybe a cumulative sum of them) alongside with transpose block structure?\n   Then we will only need to perform splitting C column blocks into T sets will cost only T log(C), avoiding O(C) computation of number of non-zeros\n - I think that a similar splitting workload into equal chunks might be needed for left multiplication in partitioned matrix view. Thus it might be useful to separate this functionality into something like ParallelFor(context, from, to, computation_cost, num_threads, function), right?\n\n\n\nIn the table below linear solver and total run-times in seconds are reported for CGNR solver with Jacobi preconditioner for 10 optimization iterations.\nDatasets are sorted by decreasing total time, because on some of them first ten iterations do not result into large number of linear solver iterations,\nthus savings from linear operator optimizations are not very significant.\n\nImplementations being compared are current master branch (control), parallelized left products from this CL (cxx) and CUDA variant (cuda).\n\n`T` is \"total\", `LS` is \"linear solver\". For proposed implementation threading limit for switching to parallel implementation is set to \u003e\u003d 2 threads.\n\n```\n                  dataset threads LS.control   T.control      LS.cxx       T.cxx    LS.cuda     T.cuda LS.control.to.cxx LS.cxx.to.cuda\nproblem-13682-4456117-pre      28 604.654343  689.899293  274.938564  364.026082  46.815561 136.369615         2.1992344       5.872803\nproblem-13682-4456117-pre      16 625.624651  711.788415  327.712304  412.897049  49.550610 137.533354         1.9090667       6.613689\nproblem-13682-4456117-pre       8 706.108935  793.794761  468.484468  558.019669  55.082652 158.480280         1.5072195       8.505118\nproblem-13682-4456117-pre       4 680.153168  783.963860  733.296560  838.389996  65.593977 182.860066         0.9275281      11.179328\nproblem-13682-4456117-pre       2 801.972508  944.741331 1168.176171 1295.297128  84.818904 235.871336         0.6865167      13.772592\nproblem-13682-4456117-pre       1 904.213194 1088.308315  912.459588 1096.471709 113.394600 313.809273         0.9909625       8.046764\n  problem-1778-993923-pre      28 591.306498  612.196655  270.438620  291.002631  30.429067  57.046080         2.1864721       8.887509\n  problem-1778-993923-pre      16 608.540567  628.247414  313.110301  332.751628  30.725613  55.277585         1.9435342      10.190531\n  problem-1778-993923-pre       8 695.940579  716.718042  416.592095  437.172780  31.665361  58.278160         1.6705564      13.156082\n  problem-1778-993923-pre       4 682.997661  706.959453  754.519514  778.426492  33.487874  64.414297         0.9052087      22.531126\n  problem-1778-993923-pre       2 766.869539  796.139411 1087.188182 1117.696179  36.530909  74.524428         0.7053696      29.760776\n  problem-1778-993923-pre       1 818.310252  860.199554  823.812240  865.549156  41.313313  90.996053         0.9933213      19.940600\n   problem-356-226730-pre      28 212.015849  217.083578   96.719694  101.791512  11.852062  21.353406         2.1920649       8.160579\n   problem-356-226730-pre      16 223.483871  228.573042  110.502900  115.595623  11.974387  21.485999         2.0224254       9.228272\n   problem-356-226730-pre       8 235.287405  240.865757  152.474484  158.040024  12.287443  22.425180         1.5431264      12.408968\n   problem-356-226730-pre       4 252.516532  259.419594  233.054584  239.783137  12.841789  24.541246         1.0835081      18.148140\n   problem-356-226730-pre       2 281.873616  290.945671  398.124656  406.606453  13.880464  27.206398         0.7080034      28.682374\n   problem-356-226730-pre       1 302.183112  314.360864  298.872062  311.034319  14.414105  31.941241         1.0110785      20.734694\n    problem-257-65132-pre      28  38.281180   39.253325   20.013955   21.057554   2.732508   7.964920         1.9127244       7.324390\n    problem-257-65132-pre      16  46.223804   47.194197   21.751313   22.718769   2.721205   7.947704         2.1251041       7.993265\n    problem-257-65132-pre       8  38.809445   39.797179   28.271835   29.282991   2.754694   8.109363         1.3727247      10.263149\n    problem-257-65132-pre       4  45.473107   46.655625   40.521821   41.698243   2.818763   8.385292         1.1221881      14.375746\n    problem-257-65132-pre       2  51.246165   52.835401   64.026848   65.618155   2.962153   8.903465         0.8003856      21.614970\n    problem-257-65132-pre       1  55.125097   57.251099   55.233014   57.287522   3.132250   9.590666         0.9980462      17.633654\n  problem-1723-156502-pre      28   3.818617    6.062343    2.258291    4.552049   0.870070   7.471585         1.6909322       2.595528\n  problem-1723-156502-pre      16   3.906417    6.082002    2.314614    4.644365   0.917755   7.483594         1.6877186       2.522039\n  problem-1723-156502-pre       8   4.338724    6.713161    3.007280    5.450828   1.026098   7.945983         1.4427403       2.930792\n  problem-1723-156502-pre       4   4.781902    7.726164    5.055707    7.943834   1.290954   8.697052         0.9458424       3.916257\n  problem-1723-156502-pre       2   5.630870    9.033226    7.283197   10.807443   1.774227   9.960510         0.7731316       4.104997\n  problem-1723-156502-pre       1   6.097798   10.825314    6.073991   10.814352   2.426167  11.762843         1.0039195       2.503534",
      "revId": "4538392dc1ee05e8c441a93ad668d0d700002e37",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "e501c07f_7229b4f2",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5010
      },
      "writtenOn": "2022-10-24T11:58:38Z",
      "side": 1,
      "message": "wow dmitriy, this looks really interesting. I will send you a full review in the next few hours.",
      "revId": "4538392dc1ee05e8c441a93ad668d0d700002e37",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "af3b6d9d_576eefa3",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-10-24T13:48:52Z",
      "side": 1,
      "message": "Dmitriy,\nBefore I review the code I wanted to comment on your observation that the random matrix being used in the spmv benchmarks are not reflective of BAL problems. This is an important observation and worthy of being addressed sooner rather than later.\n\nI have filed https://github.com/ceres-solver/ceres-solver/issues/902 to track this and will work on this shortly.",
      "revId": "4538392dc1ee05e8c441a93ad668d0d700002e37",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "6910f6c6_6322c386",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-10-24T16:16:05Z",
      "side": 1,
      "message": "I am trying to understand your numbers.  So I am trying one of the problems myself. First some baseline numbers.\n\n\nCeres at HEAD on my mac with problem-1778-993923-pre.txt\n\n1 thread\nPreprocessor                         0.464052\n\n  Residual only evaluation           2.197948 (10)\n  Jacobian \u0026 residual evaluation     7.617696 (8)\n  Linear solver                    252.885207 (10)\nMinimizer                          264.878859\n\nPostprocessor                        0.040558\nTotal                              265.383469\n\n10 threads\nPreprocessor                         0.515728\n\n  Residual only evaluation           0.510388 (10)\n  Jacobian \u0026 residual evaluation     1.931564 (8)\n  Linear solver                    174.145458 (10)\nMinimizer                          178.850336\n\nPostprocessor                        0.047150\nTotal                              179.413214\n\nOn my linux box\n1 thread\nPreprocessor                         2.923402\n\n  Residual only evaluation           5.462493 (10)\n  Jacobian \u0026 residual evaluation    27.160345 (8)\n  Linear solver                    855.228743 (10)\nMinimizer                          895.325036\n\nPostprocessor                        0.430332\nTotal                              898.678770\n\n10 threads: (this maybe affected by the presence of hyper threading)\nPreprocessor                         3.094938\n\n  Residual only evaluation           1.932398 (10)\n  Jacobian \u0026 residual evaluation     7.941716 (8)\n  Linear solver                    609.993293 (10)\nMinimizer                          629.474766\n\nPostprocessor                        0.444029\nTotal                              633.013735\n\nSo for the linear solver we see that with 10 threads on the mac we get a speedup of 1.45x and 1.4x.  \n\n\nComparing this to your numbers.\n\nproblem-1778-993923-pre 28 591.306498 612.196655 270.438620 291.002631 \nproblem-1778-993923-pre 8 695.940579 716.718042 416.592095 437.172780\nproblem-1778-993923-pre 1 818.310252 860.199554 823.812240 865.549156  \n\n8 threads  at HEAD 818/695 ~ 1.2x and with your changes 823/416 ~ 2x\n28 threads at HEAD 818/591 ~ 1.4x and with your changes 823/270 ~ 3x \n\nis this a correct interpretation?",
      "parentUuid": "67d0d2b9_797e0232",
      "revId": "4538392dc1ee05e8c441a93ad668d0d700002e37",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "71daacd7_55f3f9cf",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 6141
      },
      "writtenOn": "2022-10-24T16:33:25Z",
      "side": 1,
      "message": "Yes, your interpretation is correct.\n\nFor this part of table linear solver took \n\n```\n     1-core 8-core 28-core\nHEAD 818s   695s   591s\nCL   823s   416s   279s\n```\n\n\nDividing (818+823)/2 by this table yields speedup over single-threaded as\n\n```\n     1-core 8-core 28-core\nHEAD 1.00x  1.18x  1.39x\nCL   1.00x  1.97x  2.94x\n```\n\n\nTime is measured on 28-core/56-thread CPU inside a dual-cpu workstation (running debian sid), with a total memory bandwidth per cpu ~120Gbyte/s.\n\nThreads are pinned to a single cpu and all memory allocations are pinned to the memory attached to said CPU.",
      "parentUuid": "6910f6c6_6322c386",
      "revId": "4538392dc1ee05e8c441a93ad668d0d700002e37",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    }
  ]
}