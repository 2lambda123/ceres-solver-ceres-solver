{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "3915095e_677e9267",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 6141
      },
      "writtenOn": "2023-05-05T15:59:08Z",
      "side": 1,
      "message": "Here we start with stuff for GPGPU implicit Schur complement.\n\n\nThe first thing is block-sparse to CRS device-side remapping using pre-computed permutation.\n\n\nAs we have discussed earlier, switching to CRS jacobians is infeasible in case of implicit Schur complement.\n\nProposed solution keeps host-side matrices in block-sparse format, and uses a pre-computed permutation from block-sparse to CRS order of values for updating CRS matrix on gpu side.\n\nThis CL provides classes that are re-used for the case of partitioned matrix:\n * `CudaBlockSparseStructure` -- used to store block structure as a couple of arrays on device side\n * `CudaStreamer` -- used for simultaneous host-to-device copy and processing data\n\n\n\nBenchmark results:\n\n```\n./bin/evaluation_benchmark ./venice/problem-{1778*,52*} --benchmark_filter\u003d\u0027.*ToCRS.*\u0027\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d CUDA Device Properties \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nCuda version         : 11.8\nDevice ID            : 0\nDevice name          : NVIDIA GeForce RTX 2080 Ti\nTotal GPU memory     :  11012 MiB\nGPU memory available :  10852 MiB\nCompute capability   : 7.5\nWarp size            : 32\nMax threads per block: 1024\nMax threads per dim  : 1024 1024 64\nMax grid size        : 2147483647 65535 65535\nMultiprocessor count : 68\nRunning ./bin/evaluation_benchmark\nRun on (112 X 3200 MHz CPU s)\nCPU Caches:\n  L1 Data 32 KiB (x56)\n  L1 Instruction 32 KiB (x56)\n  L2 Unified 1024 KiB (x56)\n  L3 Unified 39424 KiB (x2)\n-----------------------------------------------------------------------------\nBenchmark                                                                Time\n-----------------------------------------------------------------------------\nJacobianToCRS\u003c./venice/problem-1778-993923-pre.txt\u003e                    905 ms\nJacobianToCRSView\u003c./venice/problem-1778-993923-pre.txt\u003e                374 ms\nJacobianToCRSMatrix\u003c./venice/problem-1778-993923-pre.txt\u003e             1247 ms\nJacobianToCRSViewUpdate\u003c./venice/problem-1778-993923-pre.txt\u003e          199 ms\nJacobianToCRSMatrixUpdate\u003c./venice/problem-1778-993923-pre.txt\u003e        219 ms\n\nJacobianToCRS\u003c./venice/problem-52-64053-pre.txt\u003e                      56.1 ms\nJacobianToCRSView\u003c./venice/problem-52-64053-pre.txt\u003e                  40.8 ms\nJacobianToCRSMatrix\u003c./venice/problem-52-64053-pre.txt\u003e                82.1 ms\nJacobianToCRSViewUpdate\u003c./venice/problem-52-64053-pre.txt\u003e            14.5 ms\nJacobianToCRSMatrixUpdate\u003c./venice/problem-52-64053-pre.txt\u003e          14.7 ms\n```\n\nNew benchmarks:\n\n * `JacobianToCRS` \u003d convert block-sparse to CRS on CPU\n * `JacobianToCRSView` \u003d convert block-sparse to CRS on GPU and compute permutation\n * `JacobianToCRSMatrix` \u003d convert block-sparse to CRS on CPU and copy to GPU\n * `JacobianToCRSViewUpdate` \u003d update CRS values on GPU using block-sparse values on CPU and pre-computed permutation\n * `JacobianToCRSMatrixUpdate` \u003d update CRS values on GPU using CRS values on CPU\n\nObserved behavior:\n - `JacobianToCRSView` takes less time than both `JacobianToCRS` and `JacobianToCRSMatrix` (thus, GPU-side creation of CRS structure is better)\n - `JacobianToCRSViewUpdate` takes +- the same time as \n `JacobianToCRSMatrixUpdate` (thus, time spend shuffling values is hidden by pcie latency) [actually, slightly less in both cases; I guess that is due to larger buffer being used in driver]\n\n\n\nThings we need to discuss:\n * Ubuntu 22.04 ships cuda toolkit and gcc versions that are known to be incompatible ( https://bugs.debian.org/cgi-bin/bugreport.cgi?bug\u003d1006962 ); this can be solved in multiple way:\n   * Use latest-and-greatest cuda toolkit from nvidia (implemented in CL)\n   * Implement prefix sum ourselves\n   * Compute prefix sum of non-zeros per row-block and copy results to gpu when creating a deep copy of block structure\n   * Downgrading CUDA_HOST_COMPILER to gcc-10 might also help\n * The process of updating values is limited by block-sparse values being allocated in pageable memory (thus, requiring additional copy on cpu side in order to be able to make a DMA copy to gpu). With block-sparse values allocated in non-pageable memory value updates are almost 2x faster on my pc.",
      "revId": "1572dbbf74f3f5e6064bde89a71f35f3a661459d",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    }
  ]
}