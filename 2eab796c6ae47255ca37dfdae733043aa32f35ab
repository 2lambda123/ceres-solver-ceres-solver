{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "5899b5ef_d996085d",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 6141
      },
      "writtenOn": "2022-09-08T11:38:48Z",
      "side": 1,
      "message": "The first part of bringing multi-threading to iterative linear solvers with partitioned matrix view and block-sparse matrices.\n\nThis provides a reasonable speedup for right products.\nWith higher core-counts ratio w.r.t GPGPU approach is close to memory throughput ratio (120GB/s vs 620GB/s in my setup).\n\nNote that a parallel-for implementation with non-interleaving indexes *is a must* to obtain good speedups; with interleaved indexes in ParallelFor performance gains are minuscule.",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "f5a9331d_e94fc505",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5010
      },
      "writtenOn": "2022-09-08T15:04:34Z",
      "side": 1,
      "message": "Thank you Dmitriy, this is very interesting. \nGiven that we are still going through figuring out which threadpool to use, I think we should review and submit this CL and use this as part of the evaluation of your change to the threadpool and what Mike is proposing.\n\nWDYT?",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "203496d3_184066c3",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5010
      },
      "writtenOn": "2022-09-08T15:06:21Z",
      "side": 1,
      "message": "I will review the CL later today. I also have another change which may impact this CL in flight https://ceres-solver-review.googlesource.com/c/ceres-solver/+/22760.",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "cf8d9069_8cb294d5",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 6141
      },
      "writtenOn": "2022-09-08T15:40:24Z",
      "side": 1,
      "message": "I think we can review and submit this one first; as long as callers do not provide a valid ContextImpl -- it should work as good as it was with sequential methods.\nThose changes do not depend on particular threadpool implementation per se, but only work great with those that do not supply nearby indices to different threads (for example, I had success with tbb-based one as I\u0027ve mentioned in google groups a while ago).\n\nSo far I\u0027ve only added setting parallel context into tests.\nI think it makes sense to get both left- and right- products multi-threaded before implementing setting parallel context in all iterative solvers.",
      "parentUuid": "f5a9331d_e94fc505",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "694b2f9b_17ddf9ea",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 6141
      },
      "writtenOn": "2022-09-08T15:40:24Z",
      "side": 1,
      "message": "I am ready to rebase over those changes if it will be easier this way.",
      "parentUuid": "203496d3_184066c3",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "1a2b02d5_8b29fae4",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5010
      },
      "writtenOn": "2022-09-08T15:43:02Z",
      "side": 1,
      "message": "I am still waiting on a review of my other change. But thats okay, I will take a detailed look at yours later today. My day is a bit bushed, but I should have time later in the evening today.",
      "parentUuid": "694b2f9b_17ddf9ea",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "1e40021e_abcd9037",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-08T21:32:55Z",
      "side": 1,
      "message": "I have some minor comments but one big one.\n\nThat has to do with SetContext and SetNumThreads.\n\nFirst of all, having two different methods means that you can call SetNumThreads while Context is null, which leaves you in a weird state.\n\nBut this raises the question, why should matrices have any thread and context objects at all.  \n\nI think the right thing to do is have the left and right multiply methods have threaded variants. WDYT?",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "9a5e588d_95b7b816",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 6141
      },
      "writtenOn": "2022-09-09T09:22:22Z",
      "side": 1,
      "message": "My decision to make matrices hold a pointer to ContextImpl object inside was mainly driven by CudaSparseMatrix storing a pointer to ContextImpl.\n\nThis definitely can be also implemented by passing a context object only to methods that utilize it, since not every method requires it.\n\nIf I change implementation for cpu-side matrices to accepting pointers to context only in methods that utilize it - should gpu-side matrix classes be changed in the same way?",
      "parentUuid": "1e40021e_abcd9037",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "2d2a7bf1_349f87ab",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-11T14:49:27Z",
      "side": 1,
      "message": "Thats a good point, I had not thought about CudaSparseMatrix. In light of that decision it makes sense to have the Context in the matrix. Then the only thing that remains is the right way to inject the Context into the matrix and set the number of threads.\n\nShould this be a construction time thing, or should it be something that happens afterwards. Both of them have their ups and downs.\n\nI think the former is better, because once constructed the user does not have to worry about whether the matrix has the requisite capabilities or not. This may require that we have to pipe the context object more deeply in some places but I think if we if we allow injection of these objects later on, then it is not clear who is responsible for doing so.\n\nAnd to make sure that we are doing this right and not missing anything, I would not have constructors with default arguments and instead carefully modify all callsites.\n\nWDYT?",
      "parentUuid": "9a5e588d_95b7b816",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "52279d9d_ce3cf94f",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-14T03:22:57Z",
      "side": 1,
      "message": "FYI",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "759ae621_3b3a4a6d",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5135
      },
      "writtenOn": "2022-09-14T12:08:42Z",
      "side": 1,
      "message": "In the Cuda* objects, the design I\u0027d like to move towards is that all objects get a ContextImpl* on construction, with the assumption that it has already been initialized. Constructors only perform a lightweight check to see if it is already initialized, and fail if not initialized. Since the context has all the handles that the CUDA operations need, all objects have consistent usage of resources.\n\nCan the number of threads be absorbed into the Context? If this needs to vary based on the compute being performed, it would be good to have something like a ThreadingPolicy under the context that returns (for example) the number of threads to use for different classes of operations.",
      "parentUuid": "2d2a7bf1_349f87ab",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "0ee3f8ba_0e1cf62e",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5135
      },
      "writtenOn": "2022-09-14T12:08:42Z",
      "side": 1,
      "message": "Great to see threading in the CPU SpMV operations!\n@Dmitriy, could you please add the CPU and GPU model numbers to the commit message?",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "aed4cbc1_0b786ad2",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5010
      },
      "writtenOn": "2022-09-14T13:45:48Z",
      "side": 1,
      "message": "I think context and threading are qualitatively different things. Context is passed to the problem object and is basically state which is expensive to carry, whereas num threads come from the solve call.\n\nI do not think they can or should be merged into one.\n\nSameer",
      "tag": "mailMessageId\u003d\u003cCAK0oyEoQ8dt0GjB6aURMm12DBj-5U2BOaa7kV+vc4LC9LdpMKQ@mail.gmail.com\u003e",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "0cf41311_e9fc0ea1",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5135
      },
      "writtenOn": "2022-09-14T13:54:32Z",
      "side": 1,
      "message": "Doesn\u0027t the context already manage the CXX thread pool? \nIs the plan to remove it from the context once the new threading model is implemented?",
      "parentUuid": "aed4cbc1_0b786ad2",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "9f4dd12c_b5e45dcf",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-14T13:56:41Z",
      "side": 1,
      "message": "It manages the thread pool, but it does not decide number of threads used to solve. The user can decide that independent of problem construction.",
      "tag": "mailMessageId\u003d\u003cCABqdRUD4\u003dQDqCM+\u003d4ePobT5p8PmnfpcvB5C8fUCfULv8mTV0+w@mail.gmail.com\u003e",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "e95ba6ed_5cef1aa7",
        "filename": "internal/ceres/block_sparse_matrix.cc",
        "patchSetId": 1
      },
      "lineNbr": 52,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-08T21:32:55Z",
      "side": 1,
      "message": "block -\u003e row_block_id",
      "range": {
        "startLine": 52,
        "startChar": 14,
        "endLine": 52,
        "endChar": 19
      },
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "6e896051_951e3339",
        "filename": "internal/ceres/block_sparse_matrix.cc",
        "patchSetId": 1
      },
      "lineNbr": 55,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-08T21:32:55Z",
      "side": 1,
      "message": "all these ints can be marked const no?",
      "range": {
        "startLine": 55,
        "startChar": 6,
        "endLine": 55,
        "endChar": 19
      },
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "4310c155_267ce9c1",
        "filename": "internal/ceres/block_sparse_matrix.cc",
        "patchSetId": 1
      },
      "lineNbr": 132,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-08T21:32:55Z",
      "side": 1,
      "message": "parallel for is perfectly capable of handling the 1 thread case, so why have two variants here?",
      "range": {
        "startLine": 132,
        "startChar": 4,
        "endLine": 132,
        "endChar": 15
      },
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "90813c55_f2571ae0",
        "filename": "internal/ceres/block_sparse_matrix.cc",
        "patchSetId": 1
      },
      "lineNbr": 132,
      "author": {
        "id": 6141
      },
      "writtenOn": "2022-09-09T09:22:22Z",
      "side": 1,
      "message": "a. ParallelFor requires a valid context (maybe we can just move the check for context pointer lower inside ParallelFor implementation though).\n\n\nb. For small loops (which can be as small as computing 2x3 matrix by 3-vector product in the case of PartitionedMatrixView::RightMultiplyE) there is a very high penalty for wrapping loop body into std::function compared to \"perfect\" inlining of a local function (up to 15% for jacobians from BA problems, up to 5% for unstructured ones, ~5% for partitioned view).\nI\u0027ve tried to keep single-threaded performance unchanged.\n\n\nHere are timings with a separate single-threaded case with inlined loop body:\nRunning ./bin/spmv_benchmark\n----------------------------------------------------------------------\nBenchmark                                                         Time\n----------------------------------------------------------------------\nBM_BlockSparseRightMultiplyAndAccumulateBA/1_mean              28.2 ms\nBM_BlockSparseRightMultiplyAndAccumulateBA/1_median            28.2 ms\nBM_BlockSparseRightMultiplyAndAccumulateBA/1_stddev           0.186 ms\nBM_BlockSparseRightMultiplyAndAccumulateBA/1_cv                0.66 % \nBM_BlockSparseRightMultiplyAndAccumulateUnstructured/1_mean    24.4 ms\nBM_BlockSparseRightMultiplyAndAccumulateUnstructured/1_median  24.4 ms\nBM_BlockSparseRightMultiplyAndAccumulateUnstructured/1_stddev 0.225 ms\nBM_BlockSparseRightMultiplyAndAccumulateUnstructured/1_cv      0.92 % \nBM_CRSRightMultiplyAndAccumulateBA/1_mean                      23.8 ms\nBM_CRSRightMultiplyAndAccumulateBA/1_median                    23.8 ms\nBM_CRSRightMultiplyAndAccumulateBA/1_stddev                   0.053 ms\nBM_CRSRightMultiplyAndAccumulateBA/1_cv                        0.22 % \nBM_CRSRightMultiplyAndAccumulateUnstructured/1_mean            20.6 ms\nBM_CRSRightMultiplyAndAccumulateUnstructured/1_median          20.6 ms\nBM_CRSRightMultiplyAndAccumulateUnstructured/1_stddev         0.093 ms\nBM_CRSRightMultiplyAndAccumulateUnstructured/1_cv              0.45 % \n\nRunning ./bin/partitioned_matrix_view_benchmark\n----------------------------------------------------------------------\nBenchmark                                                        Time \n----------------------------------------------------------------------\nBM_PatitionedViewRightMultiplyAndAccumulateE_Static/1_mean     18.3 ms \nBM_PatitionedViewRightMultiplyAndAccumulateE_Static/1_median   18.4 ms \nBM_PatitionedViewRightMultiplyAndAccumulateE_Static/1_stddev  0.039 ms \nBM_PatitionedViewRightMultiplyAndAccumulateE_Static/1_cv       0.21 %  \nBM_PatitionedViewRightMultiplyAndAccumulateF_Static/1_mean     18.9 ms \nBM_PatitionedViewRightMultiplyAndAccumulateF_Static/1_median   18.9 ms \nBM_PatitionedViewRightMultiplyAndAccumulateF_Static/1_stddev  0.150 ms \nBM_PatitionedViewRightMultiplyAndAccumulateF_Static/1_cv       0.79 % \n\nHere are timings with single-threaded case being handled with ParallelFor:\n----------------------------------------------------------------------\nBenchmark                                                         Time\n----------------------------------------------------------------------\nBM_BlockSparseRightMultiplyAndAccumulateBA/1_mean              31.9 ms\nBM_BlockSparseRightMultiplyAndAccumulateBA/1_median            31.9 ms\nBM_BlockSparseRightMultiplyAndAccumulateBA/1_stddev           0.211 ms\nBM_BlockSparseRightMultiplyAndAccumulateBA/1_cv                0.66 % \nBM_BlockSparseRightMultiplyAndAccumulateUnstructured/1_mean    24.9 ms\nBM_BlockSparseRightMultiplyAndAccumulateUnstructured/1_median  24.8 ms\nBM_BlockSparseRightMultiplyAndAccumulateUnstructured/1_stddev 0.147 ms\nBM_BlockSparseRightMultiplyAndAccumulateUnstructured/1_cv      0.59 % \nBM_CRSRightMultiplyAndAccumulateBA/1_mean                      27.2 ms\nBM_CRSRightMultiplyAndAccumulateBA/1_median                    27.1 ms\nBM_CRSRightMultiplyAndAccumulateBA/1_stddev                   0.207 ms\nBM_CRSRightMultiplyAndAccumulateBA/1_cv                        0.76 % \nBM_CRSRightMultiplyAndAccumulateUnstructured/1_mean            22.0 ms\nBM_CRSRightMultiplyAndAccumulateUnstructured/1_median          22.0 ms\nBM_CRSRightMultiplyAndAccumulateUnstructured/1_stddev         0.063 ms\nBM_CRSRightMultiplyAndAccumulateUnstructured/1_cv              0.29 % \n\nRunning ./bin/partitioned_matrix_view_benchmark\n----------------------------------------------------------------------\nBenchmark                                                        Time \n----------------------------------------------------------------------\nBM_PatitionedViewRightMultiplyAndAccumulateE_Static/1_mean    19.5 ms \nBM_PatitionedViewRightMultiplyAndAccumulateE_Static/1_median  19.5 ms \nBM_PatitionedViewRightMultiplyAndAccumulateE_Static/1_stddev 0.068 ms \nBM_PatitionedViewRightMultiplyAndAccumulateE_Static/1_cv      0.35 %  \nBM_PatitionedViewRightMultiplyAndAccumulateF_Static/1_mean    19.8 ms \nBM_PatitionedViewRightMultiplyAndAccumulateF_Static/1_median  19.7 ms \nBM_PatitionedViewRightMultiplyAndAccumulateF_Static/1_stddev 0.153 ms \nBM_PatitionedViewRightMultiplyAndAccumulateF_Static/1_cv      0.78 %  \n\n\nAs one may note, there is always a [statistically] significant difference in favor of inlined loop body, while standard deviations are pretty small.",
      "parentUuid": "4310c155_267ce9c1",
      "range": {
        "startLine": 132,
        "startChar": 4,
        "endLine": 132,
        "endChar": 15
      },
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "e34cd8b2_cd57e624",
        "filename": "internal/ceres/block_sparse_matrix.cc",
        "patchSetId": 1
      },
      "lineNbr": 132,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-11T14:49:27Z",
      "side": 1,
      "message": "thank you Dmitriy for these experiments make sense to me. I imagine having a lambda defined right where the parallelfor loop is used is not going to make things any better either. This is what is done elsewhere where we use parallelfor and this maybe worthy of some inspection in light of your experiments.",
      "parentUuid": "90813c55_f2571ae0",
      "range": {
        "startLine": 132,
        "startChar": 4,
        "endLine": 132,
        "endChar": 15
      },
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "ddf3e773_2f2fb905",
        "filename": "internal/ceres/block_sparse_matrix.h",
        "patchSetId": 1
      },
      "lineNbr": 141,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-08T21:32:55Z",
      "side": 1,
      "message": "drop blank",
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "8d6bd44f_20306f57",
        "filename": "internal/ceres/block_sparse_matrix.h",
        "patchSetId": 1
      },
      "lineNbr": 143,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-08T21:32:55Z",
      "side": 1,
      "message": "The default number of threads should be 1 no? what does zero mean?",
      "range": {
        "startLine": 143,
        "startChar": 22,
        "endLine": 143,
        "endChar": 23
      },
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a7e1b3ed_be2934c9",
        "filename": "internal/ceres/compressed_row_sparse_matrix.cc",
        "patchSetId": 1
      },
      "lineNbr": 316,
      "author": {
        "id": 5002
      },
      "writtenOn": "2022-09-08T21:32:55Z",
      "side": 1,
      "message": "same comment as block_sparse_matrix.",
      "range": {
        "startLine": 316,
        "startChar": 6,
        "endLine": 316,
        "endChar": 19
      },
      "revId": "2eab796c6ae47255ca37dfdae733043aa32f35ab",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    }
  ]
}