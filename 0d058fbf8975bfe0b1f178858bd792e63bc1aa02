{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "3e42f579_53bd423d",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 6
      },
      "lineNbr": 0,
      "author": {
        "id": 5002
      },
      "writtenOn": "2020-11-30T00:54:47Z",
      "side": 1,
      "message": "Evan,\n\nSorry for taking so long on this, but I finally had time to go through the references and your derivation. Some thoughts/questions.\n\nAs a minor aside, the papers are exceptionally poorly written and your derivation in the documentation is infinitely easier to read and to the point, so thank you for that.\n\nIf we forget about robust norms for now, then basically what you are doing is the following.\n\nConsider the simplified two variable optimization \n\nmin_x,y sum_i f^2_i(x,y)\n\nand we want to eliminate/marginalize x.\n\nthen given linearization points x_0, y_0 we can replace the nonlinear optimization problem by its linearization\n\nmin_{dx, dy} sum_i (f_i(x_0, y_0) + J_i [dx;dy])^2\n\nwhich we can then write as a two step optimization problem as \n\nmin_{dy} min_{dx} sum_i (f_i(x_0, y_0) + J_i [dx;dy])^2\n\nwhich we can then replace by something of the form\n\nmin_{dy} |A dy + b|^2\n\nwhere A and b are obtained by solving the inner optimization problem w.r.t dx.\n\nwhich we can then re-write in terms of the original variables as:\n\nmin_{y} |A (y-y_0) + b|^2\n \nThis lines up with my understanding of classical marginalization. I think the papers you cite have some sort of sparsification logic (something involving trees) in them, but thats not being done here. \n\nIs this correct? I am going entirely off of your documentation right now, or is there more going on in the code?\n\nAssuming my understanding is correct, a couple more questions/concerns.\n\n1. The tranformation dx \u003d y - y_0 is problematic and points to the gap in the way ceres handles manifolds, and the lack of an ominus operator. Without that its not clear how you are able to do this a meaningful manner?\n\n2. There is an assumption that the minimization w.r.t dx actually results in a value thats less than \\sum_i f_i(x_0, y_0), i.e. it is better than the linearization point. This may or may not be true. I suppose this is not something that is being handled.\n\n3. In your derivation (which is not reflected in the simplified derivation above) you are assuming that the robust loss rho_i is being applied via its derivative, this is not always true -- it depends on how things are implemented. I think the derivation can be done to handle this in a more general manner.\n\n4. What happens if there are bounds constraints on x?\n\nYour CL is starting to make me think that it is time to think seriously about updating the handling of manifolds in Ceres Solver before building more stuff up on top of what is only half of what needs to exist.\n\nSameer\n",
      "revId": "0d058fbf8975bfe0b1f178858bd792e63bc1aa02",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "32d7fc9f_0b2b8856",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 6
      },
      "lineNbr": 0,
      "author": {
        "id": 5002
      },
      "writtenOn": "2020-12-01T00:00:36Z",
      "side": 1,
      "message": "Evan,\nMy replies are inline.\n\n\u003e Patch Set 6:\n\u003e \n\u003e Thanks, Sameer. This is correct-- the content on sparsification is tangential. Perhaps these are useful extensions, but they are not required.\n\nGood to know. I agree starting from the basics is the way to go.\n \n\u003e Sorry if I created some confusion here, but I think that in your derivation, there is a difference related to manifolds, or maybe I have a different interpretation of it. There is an asymmetry between dx and dy-- when we compute this cost function, we minimize over dx \u003d x ominus x0, the local coordinates, while we keep y - y0 in global coordinates throughout the derivation.\n\u003e \n\u003e If we take y to be on a manifold M, I would modify your derivation to be:\n\u003e \n\u003e min_{dx, y \\in M}\n\u003e \n\u003e \u003d min_{y \\in M} min_{dx} (f_i(x_0, y_0) + J_i [dx; y - y0])^2\n\u003e \n\u003e \u003d min_{y \\in M} |A(y-y0) + b|^2,\n\u003e \n\u003e Here, we can minimize over dx and, I believe, meaningfully represent the marginal regardless of the local parameterization of y. So subsequent optimizations solve at each step\n\u003e \n\u003e min_{dy} | A ( y oplus dy - y0 ) + b |^2\n\nI do not think what I did is all that different from what you are doing. To be able to keep things as\ny - y0 would require that you have the Jacobian w.r.t y in the ambient space. Why would you do that? also in my derivation you could just write it as\n\nmin_{dx, dy} sum_i (f_i(x_0, y_0) + J_i [dx;ominus(y, y_0)])^2\n\nand everything carries through. \n\n\u003e \n\u003e I believe this is what Carlevaris-Bianco et al. is proposing, but I am not sure that either if any of these papers are explicit about local vs global coordinates. Eckenhoff et al. has a relatively detailed derivation, but it only seems implicit that \"x\" variables are minimal. I used the same notation, but my xm is not minimal.\n\u003e \n\u003e I actually just finished cleaning up an implementation of the alternate formulation where y - y0 is instead y ominus y0 in the cost function. In that approach, the steps are analogous, and J_i has the Jacobian of the local parameterization for y. This seems to be far more common and also natural. To your point, the major change is that the local parameterizations used in marginalization to implement an interface that has ominus and its Jacobian. It is not strictly necessary to make changes outside of the marginalization code. Instead, I can just cast the parameterization and call ominus. I also have an autodiff option analogous to AutodiffLocalParamerization for the ominus Jacobian, and this was fairly straightforward. Let me know if it would be useful for me to share this to get a sense of what would be involved. This approach is far more common, and maybe it is the right one to start with.\n\nI do not completely understand this. Could you write it up as a simple example ?\n\n\n\u003e 1. Tried to answer this above.\n\u003e \n\u003e 2. Correct. I am not sure there is a way to handle this. Perhaps there should be an option to check this and make it part of the success criteria. Access to intermediate results could allow the user to evaluate how good the approximation is before committing to it.\n\nI have not seen anyone really deal with this, and I am not sure this should be blocker. I was just raising this as a blind spot in these derivations.\n\n\u003e \n\u003e 3. I see. I will see if I can generalize the handling of robustifiers in the code and derivation.\n\nI think you can just assume that the residual and the Jacobians account for the robustifier as well as the local parameterization and everything carries through.\n\n\n\u003e 4. I was also thinking about this recently. I can see two additional success criteria to add for marginalization-- 1) the initial state (x0, y0) is feasible, and 2) the minimizer dx is feasible. On a related note, the current implementation does not allow variables to be fixed.\n\nbut the minimizer dx being feasible will lead to general inequality constraints on dy, which ceres does not support right now and will lead to a lot of problem.\n\nSameer\n\n\n\u003e \n\u003e Evan\n\n",
      "revId": "0d058fbf8975bfe0b1f178858bd792e63bc1aa02",
      "serverId": "cdcfcfbe-2044-3b1f-b5d4-da77ad542329"
    }
  ]
}